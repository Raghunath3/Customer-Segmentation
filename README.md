# üõí Customer Segmentation and Classification Project

## üéØ Project Overview
This project delivers a complete pipeline for **unsupervised customer segmentation** followed by a **supervised classification** model to categorize new customers instantly. The goal is to provide actionable insights into customer behavior for targeted marketing and resource allocation.

The application is built using **Streamlit**, allowing users to input customer features and receive an immediate prediction of their segment (e.g., 'Target', 'Miser', 'Spender').

## üåê Live Application
The interactive web application is deployed and accessible at:
üëâ **[https://customer-segments.streamlit.app/](https://customer-segments.streamlit.app/)**

## üíª Technical Stack & Key Files

| Component | Technology/Library | Purpose |
| :--- | :--- | :--- |
| **Data Science** | Python, NumPy, Pandas, Matplotlib, Scikit-learn | Core data manipulation, modeling, and visualization. |
| **Clustering** | **K-Means** | Unsupervised learning to identify natural customer groups. |
| **Classification** | **Decision Tree Classifier** | Supervised model to predict the segment of new, unseen customers. |
| **Web App** | **Streamlit** | Built the interactive, user-friendly UI for deployment. |
| **Persistence** | `.pkl` and `.sav` files | Saved the trained K-Means and Decision Tree models for instant loading. |

### Repository Files
* `app.py`: The main Streamlit application script.
* `Customer Data.csv`: The initial dataset used for training.
* `kmeans_model.pkl`: The saved K-Means clustering model.
* `final_model.sav`: The saved Decision Tree classification model.
* `requirements.txt`: Lists all Python dependencies for the deployment environment.

## ‚öôÔ∏è Methodology & Steps Taken

This project followed a standard Machine Learning lifecycle:

### 1. Data Engineering & Preprocessing
* **Data Collection:** Acquired the customer dataset containing features like **Age**, **Annual Income**, and **Spending Score**.
* **Cleaning:** Handled all missing values and checked for outliers.
* **Normalization:** Applied **feature scaling** (Normalization/Standardization) to ensure all features contribute equally to the distance-based K-Means algorithm.

### 2. Modeling & Training
* **Clustering (Unsupervised):**
    * Used the **Elbow Method** to determine the optimal number of clusters, which was found to be **5**.
    * Trained the **K-Means model** to group customers into 5 distinct segments.
    * Visualized the clusters using **Matplotlib** for interpretation.
* **Classification (Supervised):**
    * Used the cluster labels generated by K-Means as the target variable.
    * Trained a **Decision Tree Classifier** to map raw input features to the final 5 customer segments.
    * Serialized and saved both the K-Means model (`.pkl`) and the Decision Tree model (`.sav`).

### 3. Deployment & Integration
* **Streamlit App Development:** Created an interactive interface (`app.py`) for users to enter new customer data.
* **Model Integration:** Ensured the Streamlit app correctly loads the saved `.pkl` and `.sav` models and uses them for real-time predictions.
* **Deployment:** Deployed the final application onto **Streamlit Cloud** for 24/7 global accessibility.
